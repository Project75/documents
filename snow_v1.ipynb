{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "d6fb32fd69316596e236eab5fb8cf77c848508c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_excel\n",
    "df = read_excel('input_dumps1_1.xlsx', sheet_name = 'Sheet1' )\n",
    "#print(df.head()) # shows headers with top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 2)\n"
     ]
    }
   ],
   "source": [
    "fields = ['Short description', 'Assignment group']\n",
    "df = pd.read_excel('user_incidents_dumps_60days.xlsx', skipinitialspace=True, usecols=fields)\n",
    "#sheet_name ='Page 1',\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx = pd.ExcelFile('user_incidents_dumps_60days.xlsx')\n",
    "fields = ['Short description', 'Assignment group']\n",
    "#df = pd.read_excel('user_incidents_dumps_60days.xlsx', skipinitialspace=True, usecols=fields)\n",
    "data_sheets = []\n",
    "for sheet in xlsx.sheet_names:\n",
    "    data_sheets.append(pd.read_excel(xlsx,sheet,skipinitialspace=True, usecols=fields))\n",
    "df = pd.concat(data_sheets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = [c.replace(' ', '_') for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Short_description</th>\n",
       "      <th>Assignment_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Network Account || Unable to login || Account ...</td>\n",
       "      <td>DH-Enterprise IT Service Cntr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ms4 -  phxasp01 - job needs to be killed / job...</td>\n",
       "      <td>DH-Enterprise IT Service Cntr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Printer || Unable to print the strips  through...</td>\n",
       "      <td>DH-NC-EUS Stockton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clairiva - unable to login / user id: ltimpog ...</td>\n",
       "      <td>DH-Enterprise IT Service Cntr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WOW || Unable to turn on || Black Screen || De...</td>\n",
       "      <td>DH-SC-EUS Bakersfield MSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Short_description  \\\n",
       "0  Network Account || Unable to login || Account ...   \n",
       "1  Ms4 -  phxasp01 - job needs to be killed / job...   \n",
       "2  Printer || Unable to print the strips  through...   \n",
       "3  Clairiva - unable to login / user id: ltimpog ...   \n",
       "4  WOW || Unable to turn on || Black Screen || De...   \n",
       "\n",
       "                Assignment_group  \n",
       "0  DH-Enterprise IT Service Cntr  \n",
       "1  DH-Enterprise IT Service Cntr  \n",
       "2             DH-NC-EUS Stockton  \n",
       "3  DH-Enterprise IT Service Cntr  \n",
       "4      DH-SC-EUS Bakersfield MSH  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43822, 2)\n"
     ]
    }
   ],
   "source": [
    "#get data with at leaset count(assignment group)>200\n",
    "df1=df.groupby(\"Assignment_group\").filter(lambda x: len(x) > 500)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DH-Enterprise IT Service Cntr        27828\n",
       "DH-Helpdesk HDAG                      1855\n",
       "DHE-HR Tier 2 - Talent Management     1612\n",
       "DH-SW-EUS StJoseph                    1311\n",
       "DH-Helpdesk RRE                       1226\n",
       "DH-MPS Kyocera                        1058\n",
       "DH-ClinApps NAS                       1047\n",
       "DH-SW-EUS Chandler                     956\n",
       "DH-NC-EUS Stockton                     765\n",
       "DH-SC-EUS StJohnRMC                    673\n",
       "DH-NC-Telcom Sacramento                624\n",
       "DH-SW-EUS Phoenix                      593\n",
       "DH-SC-EUS NLA NrthrdgRoscoe            571\n",
       "DH-NC-EUS Redding                      571\n",
       "DH-GB-EUS Dominican                    546\n",
       "DH-ClinApps CPOE                       545\n",
       "DHE-SecAdmin                           526\n",
       "DH-NC-EUS Sac MSJH                     508\n",
       "DH-SC-EUS Bakersfield BMH              504\n",
       "DH-NC-EUS Sac MGH                      503\n",
       "Name: Assignment_group, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print count of assignment groups\n",
    "df1.Assignment_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#No. of unique assignment groups =  num of output classes\n",
    "num_classes = df1.Assignment_group.nunique()\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save data to pickle format\n",
    "df1.to_pickle(\"snow_v1_43k_20out_0910.pkl\")\n",
    "#df = pd.read_pickle(\"snow_v1_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43822,) (43822,)\n"
     ]
    }
   ],
   "source": [
    "#Create input and output data sets\n",
    "X = df1.Short_description\n",
    "Y = df1.Assignment_group\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encode outputs into N- dim one hot encoded matrix\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "#print(onehot_encoded)\n",
    "# invert first example to return original, must for return api --how to inverse transform post deploy\n",
    "#inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43822, 20)\n"
     ]
    }
   ],
   "source": [
    "print(onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "df_col_len = int(df1['Short_description'].str.split().str.len().max())\n",
    "print(df_col_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_uuid": "aa3386af09469682c66cc53a1830a4e42f0e70b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37248,) (6574,) (37248, 20) (6574, 20)\n"
     ]
    }
   ],
   "source": [
    "#Split into train and test(15%)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,onehot_encoded,test_size=0.15)\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "_uuid": "bdca14f2b8cd7bd7cb5ee66fd40ea522217c03c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37248, 20)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and pad , init max sizes...should find way to eff value max words \n",
    "max_words = 5000\n",
    "max_len = 20 #df_col_len #35\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "print(sequences_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_uuid": "78fff25b8be1de575bff071a2027f3dd2b11b911",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define LSTM  model\n",
    "#tweak parameters/layers to inc. eff, smtimes 1D conv also used\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs) #50 dim\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(num_classes,name='out_layer')(layer)  #num_classes=# of outputs\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "_uuid": "a0ede32d4127e8b4990fd74fe97fadef9e565d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 20, 50)            250000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 20)                5140      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 301,220\n",
      "Trainable params: 301,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#compile model with optimizer -  can use RMSProp or Adam\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "_uuid": "98f6d6318352420ea49c532cda158f715f940f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29798 samples, validate on 7450 samples\n",
      "Epoch 1/15\n",
      "29798/29798 [==============================] - 19s 627us/step - loss: 1.2043 - acc: 0.6868 - val_loss: 0.9815 - val_acc: 0.7470\n",
      "Epoch 2/15\n",
      "29798/29798 [==============================] - 17s 564us/step - loss: 0.9045 - acc: 0.7602 - val_loss: 0.8615 - val_acc: 0.7664\n",
      "Epoch 3/15\n",
      "29798/29798 [==============================] - 17s 554us/step - loss: 0.8156 - acc: 0.7791 - val_loss: 0.8156 - val_acc: 0.7808\n",
      "Epoch 4/15\n",
      "29798/29798 [==============================] - 17s 571us/step - loss: 0.7675 - acc: 0.7887 - val_loss: 0.7874 - val_acc: 0.7883\n",
      "Epoch 5/15\n",
      "29798/29798 [==============================] - 17s 576us/step - loss: 0.7351 - acc: 0.7961 - val_loss: 0.7743 - val_acc: 0.7902\n",
      "Epoch 6/15\n",
      "29798/29798 [==============================] - 17s 561us/step - loss: 0.7087 - acc: 0.8052 - val_loss: 0.7719 - val_acc: 0.7862\n",
      "Epoch 7/15\n",
      "29798/29798 [==============================] - 16s 533us/step - loss: 0.6851 - acc: 0.8113 - val_loss: 0.7694 - val_acc: 0.7965\n",
      "Epoch 8/15\n",
      "29798/29798 [==============================] - 16s 539us/step - loss: 0.6636 - acc: 0.8176 - val_loss: 0.7620 - val_acc: 0.8017\n",
      "Epoch 9/15\n",
      "29798/29798 [==============================] - 16s 545us/step - loss: 0.6469 - acc: 0.8225 - val_loss: 0.7814 - val_acc: 0.8048\n"
     ]
    }
   ],
   "source": [
    "#execute the model, early stopping if model stops converging, batch size can be tweaked. 20% data for validation each epoch\n",
    "history = model.fit(sequences_matrix,Y_train,batch_size=64,epochs=15,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model to file\n",
    "model.save('model_snow_v1_43k_20out_0910.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##notes\n",
    "# input=24k, output classes=27,BS=64,RMS,LSTM(64,256,),dropout 0.5,epochs=9, eff=75%; if BS=128,epoch=2,eff same\n",
    "# input=55kk,output classes=57,BS=64,RMS,LSTM(64,256,),drop 0.5,epoch 5, eff=64%\n",
    "# input=43k,out classes=20,BS=64,RMS,LSTM(64,256,),drop 0.5,epoch 9, eff=80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "_uuid": "80036135a11387d952becaf2fecf653a65c02328",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create sequences for test data\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "_uuid": "3e121ab83f4a0b9f7376ab24aa25d67051171f89",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6574/6574 [==============================] - 1s 181us/step\n",
      "Test set\n",
      "  Loss: 0.750\n",
      "  Accuracy: 80.894%\n"
     ]
    }
   ],
   "source": [
    "#Calculate accuracy on test data\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3%}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 2)\n"
     ]
    }
   ],
   "source": [
    "#load unseen data for validation\n",
    "df_val = read_excel('validate2018.xlsx', sheet_name = 'Sheet1')\n",
    "Xnew=df_val.Description\n",
    "Ynew_orig = df_val.Group\n",
    "print(Xnew.shape,Ynew_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert unseen data to word tokens\n",
    "Xnew_2 = sequence.pad_sequences(tok.texts_to_sequences(Xnew),maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict output on unseen data\n",
    "ynew = model.predict(Xnew_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=zOther Clinical Application Issue || Helpdesk - Clinical Application Issue,\n",
      "Predicted=['DH-Helpdesk HDAG'],\n",
      "Original=DH-Helpdesk HDAG\n",
      "\n",
      "X=Monitor ||  How to increase the brightness || ,\n",
      "Predicted=['DH-Enterprise IT Service Cntr'],\n",
      "Original=DH-Enterprise IT Service Cntr\n",
      "\n",
      "X=Outlook || Outlook Application Issue,\n",
      "Predicted=['DH-Helpdesk RRE'],\n",
      "Original=DH-Helpdesk RRE\n",
      "\n",
      "X=Phone || Phone Issue,\n",
      "Predicted=['DH-NC-Telcom Sacramento'],\n",
      "Original=DH-NC-Telcom Sacramento\n",
      "\n",
      "X=Kyocera Printer Issue || Kyocera Printer Issue,\n",
      "Predicted=['DH-MPS Kyocera'],\n",
      "Original=DH-MPS Kyocera\n",
      "\n",
      "X=Emergency Account Disablement || user ID : aguise001 || Need to be disabled Immediately ,\n",
      "Predicted=['DHE-SecAdmin'],\n",
      "Original=DHE-SecAdmin\n",
      "\n",
      "X=Network, account lock, username : jfunk002,\n",
      "Predicted=['DH-Enterprise IT Service Cntr'],\n",
      "Original=DH-Enterprise IT Service Cntr\n",
      "\n",
      "X=printer || patient data not printing up from cerner ,\n",
      "Predicted=['DH-ClinApps NAS'],\n",
      "Original=DH-ClinApps HIM\n",
      "\n",
      "X=network account  || password reset || user name : nmatson || HDUV  done ||  reset password || issue resolved ,\n",
      "Predicted=['DH-Enterprise IT Service Cntr'],\n",
      "Original=DH-Enterprise IT Service Cntr\n",
      "\n",
      "X=Unable to print patient cardiac strips from central monitor at nurses station in TICU Trauma ICU.Other Desktop Issue || Other Desktop Issue,\n",
      "Predicted=['DH-SW-EUS StJoseph'],\n",
      "Original=DH-SW-EUS Chandler\n",
      "\n",
      "X=Cerner // Access Level is incorrectly provisioned  // FW: ***Cerner*** ,\n",
      "Predicted=['DH-ClinApps NAS'],\n",
      "Original=DHE-SecAdmin\n",
      "\n",
      "X=Label Printer Issue || Label Printer Issue,\n",
      "Predicted=['DH-Helpdesk HDAG'],\n",
      "Original=DH-NC-EUS Redding\n",
      "\n",
      "X=Trackingshell || Patient showing twice || Dept: ER || Loc: California Hospital M,\n",
      "Predicted=['DH-ClinApps NAS'],\n",
      "Original=DH-ClinApps NAS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print unseen data, predicted value, original value\n",
    "for i in range(len(Xnew)):\n",
    "\tprint(\"X=%s,\\nPredicted=%s,\\nOriginal=%s\\n\" % (Xnew[i], label_encoder.inverse_transform([argmax(ynew[i, :])]),Ynew_orig[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
